{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import helper_functions\n",
    "\n",
    "import spacy\n",
    "import serrant\n",
    "\n",
    "import re, json\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "from nltk import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "USE_L1_LEVEL = False\n",
    "\n",
    "#load the model\n",
    "model_id = '/mnt/data/samdavid/projects/projects/dissertation/training_code/t5_finetune/' + 'cowsl2h_MT5_model'\n",
    "tokenizer = MT5Tokenizer.from_pretrained('google/mt5-base')\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_id).to(torch_device)\n",
    "\n",
    "nlp = spacy.load('es')\n",
    "annotator = serrant.load('en', nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(input_text,num_return_sequences):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5, do_sample=True)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gec(input_essay):\n",
    "\n",
    "    # create temp directory\n",
    "    try:\n",
    "        os.mkdir('tmp')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    orig_lines = []\n",
    "\n",
    "    for sent in sent_tokenize(input_essay):\n",
    "        # remove original tokenize spaces\n",
    "        sent = sent.strip()\n",
    "        doc = nlp.tokenizer(sent)\n",
    "        tokens = [token.text for token in doc]\n",
    "        # whether to put utterances in the same line or not\n",
    "        orig_lines.append(\" \".join(tokens))\n",
    "\n",
    "    # predict\n",
    "    num_return_sequences = 1\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for sent in orig_lines:\n",
    "        sent_results = correct_grammar(sent, num_return_sequences)\n",
    "        results.extend(sent_results)\n",
    "        \n",
    "    print(results)\n",
    "\n",
    "    cor_lines = []\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(results):\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
    "        cor_lines.append(text)\n",
    "\n",
    "    # # generate corrections\n",
    "    edits = []\n",
    "    #add sent index to keep track of which sent the edits belong to\n",
    "    sent_index = 0\n",
    "    for orig, cor in zip(orig_lines, cor_lines):\n",
    "        orig_parse = annotator.parse(orig)\n",
    "        cor_parse = annotator.parse(cor)\n",
    "        sent_edits = annotator.annotate(orig_parse, cor_parse)\n",
    "        edits.append((orig_parse, cor_parse, sent_edits, sent_index))\n",
    "        sent_index += 1\n",
    "\n",
    "    return edits, cor_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_errors(edit_list):\n",
    "    initial_target_list = [\"R:VERB:SVA\", 'R:PREP:WC', 'M:PRON', 'R:VERB:TENSE', 'R:NOUN:NUM', 'R:VERB:FORM', 'M:PREP', 'U:PREP', 'M:VERB']\n",
    "\n",
    "    num_errors = 0\n",
    "    out_edits = []\n",
    "\n",
    "### Printing errors for testing\n",
    "    errors_tagged = []\n",
    "    for sent in edit_list:\n",
    "        for edit_item in sent[2]:\n",
    "            errors_tagged.append(edit_item.type)\n",
    "\n",
    "    print(f\"Number of errors tagged {len(errors_tagged)}\")\n",
    "    print(errors_tagged)\n",
    "### End printing errors for testing\n",
    "\n",
    "    #max errors presented per dialogue == 3 (this is something we need to test)\n",
    "    for target in initial_target_list:\n",
    "        for sent in edit_list:\n",
    "            for edit_item in sent[2]:\n",
    "                edit_type = edit_item.type\n",
    "                if num_errors >= 3:\n",
    "                    break\n",
    "                if target in edit_type:\n",
    "                    #limit number of corrections for specific target to 1 per sentence\n",
    "                    orig_sent = sent[0]\n",
    "                    cor_sent = sent[1]\n",
    "                    sent_index = sent[3]\n",
    "                    out_edits.append((orig_sent, cor_sent, edit_item, target, sent_index))\n",
    "                    num_errors += 1\n",
    "                    break\n",
    "        if num_errors >= 3:\n",
    "            break\n",
    "\n",
    "    return out_edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feedback(edit_list, l1, level):\n",
    "    errors_to_present = rank_errors(edit_list)\n",
    "    print(errors_to_present)\n",
    "    out_dict = {}\n",
    "    error_count = 0\n",
    "    \n",
    "    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "    #level = 1\n",
    "    #l1 = 'english'\n",
    "    #original = \"Yo estoy feliz con mi mismo.\"\n",
    "\n",
    "    prompt2 = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are the teacher of a Spanish {level} course. You are writing corrections for a student whose native language is {l1}. You want to provide your students with feedback about mistakes in their writing. Given an original sentence written by a student, and a corrected version of the sentence written by you, explain to the student why you made the corrections you made. Don't change either sentence. Just explain the differences between them in terms of grammar in a way a student can understand.\"),\n",
    "        (\"user\", \"Original sentence: {original}\\nCorrected Sentence: {corrected}\")\n",
    "    ])\n",
    "\n",
    "    chain2 = (    prompt2\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "             )\n",
    "\n",
    "    \n",
    "    for error in errors_to_present:\n",
    "        #unpack edit tuple\n",
    "        orig_sentence = error[0]\n",
    "        cor_sentence = error[1]\n",
    "        edit_item = error[2]\n",
    "        target = error[3]\n",
    "\n",
    "        if \"R:VERB:SVA\" in target:\n",
    "            #Don't know if I should include the full sentence\n",
    "            response_short = \"In this sentence '{orig_sent}' you made a mistake on the verb '{orig_tok}'. The correct verb form here is '{cor_tok}'. Remember to make your verbs agree with their subjects. Here's the corrected sentence: {cor_sent}\".format(orig_sent=orig_sentence.text, orig_tok=edit_item.o_str, cor_tok=edit_item.c_str, cor_sent=cor_sentence.text)\n",
    "            llm_explanation = chain2.invoke({\"l1\": l1, \"level\": level, \"original\": orig_sentence, \"corrected\": cor_sentence})\n",
    "\n",
    "            line_1 = \"In this sentence '{orig_sent}' you made a mistake on the verb '{orig_tok}'. What verb form should you have used?\".format(orig_sent=orig_sentence.text, orig_tok=edit_item.o_str)\n",
    "            response_1_correct = \"Good job. Remember to make your verbs agree with their subjects.\"\n",
    "            response_1_incorrect = \"Not quite. Think about subject-verb agreement. How should your verb be changed to agree with the subject '{subject}'?\".format(subject=helper_functions.get_subject_phrase(orig_sentence))\n",
    "            response_2_correct = \"Good job. Remember to make your verbs agree with their subjects.\"\n",
    "            response_2_incorrect = \"Good try, but not quite. It's tricky, I know. The correct verb form here is '{cor_tok}'. Remember to make your verbs agree with their subjects. Here's the corrected sentence: {cor_sent}\".format(cor_tok=edit_item.c_str, cor_sent=cor_sentence.text)\n",
    "\n",
    "            out_dict['edit_' + str(error_count)] = {\"response_short\": response_short, \"llm_explanation\": llm_explanation, \"line_1\":line_1, \"response_1\":{'correct':response_1_correct, 'incorrect':response_1_incorrect}, 'response_2':{'correct':response_2_correct, 'incorrect':response_2_incorrect}}\n",
    "            error_count += 1\n",
    "\n",
    "        elif \"R:PREP:WC\" in target:\n",
    "            #Don't know if I should include the full sentence\n",
    "            response_short = \"In this sentence '{orig_sent}' you made a mistake on the preposition '{orig_tok}', which doesn't sound natural. I'd recommend using '{cor_tok}' in this case. Here's the corrected sentence: {cor_sent}.\".format(orig_sent=orig_sentence.text, orig_tok=edit_item.o_str, cor_tok=edit_item.c_str, cor_sent=cor_sentence.text)\n",
    "            llm_explanation = chain2.invoke({\"l1\": l1, \"level\": level, \"original\": orig_sentence, \"corrected\": cor_sentence})\n",
    "            \n",
    "            line_1 = \"In this sentence '{orig_sent}' you made a mistake on the preposition '{orig_tok}', which doesn't sound natural. What other preposition should you have used?\".format(orig_sent=orig_sentence.text, orig_tok=edit_item.o_str)\n",
    "            response_1_correct = \"Great! '{cor_tok}' definitely sounds better in this sentence.\".format(cor_tok=edit_item.c_str)\n",
    "            response_1_incorrect = \"That still seems a bit off. Think about common prepositions and what might sound better here. Try one more time.\"\n",
    "            response_2_correct = \"Good job. That's the preposition I'd recommend. Sounds better, right?\"\n",
    "            response_2_incorrect = \"I still don't think that's right. I'd recommend using '{cor_tok}' in this case. Here's the corrected sentence: {cor_sent}.\".format(cor_tok=edit_item.c_str, cor_sent=cor_sentence.text)\n",
    "\n",
    "            out_dict['edit_' + str(error_count)] = {\"response_short\": response_short, \"llm_explanation\": llm_explanation, \"line_1\":line_1, \"response_1\":{'correct':response_1_correct, 'incorrect':response_1_incorrect}, 'response_2':{'correct':response_2_correct, 'incorrect':response_2_incorrect}}\n",
    "            error_count += 1\n",
    "\n",
    "        elif 'M:PRON' in target:\n",
    "            #Don't know if I should include the full sentence\n",
    "            response_short = \"You seem to be missing a pronoun in the sentence '{orig_sent}'. You should probably include '{cor_tok}' to make the sentence grammatical. Here's the corrected sentence: {cor_sent}\".format(orig_sent=orig_sentence.text, cor_tok=edit_item.c_str, cor_sent=cor_sentence.text)\n",
    "            llm_explanation = chain2.invoke({\"l1\": l1, \"level\": level, \"original\": orig_sentence, \"corrected\": cor_sentence})\n",
    "\n",
    "            line_1 = \"You seem to be missing a pronoun in the sentence '{orig_sent}'. How could you improve this sentence by adding a pronoun?\".format(orig_sent=orig_sentence.text)\n",
    "            response_1_correct = \"Yep, that's right. '{cor_tok}' is needed to make the sentence grammatical.\".format(cor_tok=edit_item.c_str)\n",
    "            response_1_incorrect = \"Not quite. Remember, prepositions and many verbs need an object like 'it' or 'him'. Try again.\"\n",
    "            response_2_correct = \"Great! '{cor_tok}' is what the sentence was missing.\".format(cor_tok=edit_item.c_str)\n",
    "            response_2_incorrect = \"You're still missing something. You should probably include '{cor_tok}' to make the sentence grammatical. Here's the corrected sentence: {cor_sent}\".format(cor_tok=edit_item.c_str, cor_sent=cor_sentence.text)\n",
    "\n",
    "            out_dict['edit_' + str(error_count)] = {\"response_short\": response_short, \"llm_explanation\": llm_explanation, \"line_1\":line_1, \"response_1\":{'correct':response_1_correct, 'incorrect':response_1_incorrect}, 'response_2':{'correct':response_2_correct, 'incorrect':response_2_incorrect}}\n",
    "            error_count += 1\n",
    "\n",
    "        elif 'R:VERB:TENSE' in target:\n",
    "            #Don't know if I should include the full sentence\n",
    "            response_short = \"The verb tense you used in '{orig_sent}' isn't quite right. You should probably use '{cor_tok}' instead of '{orig_tok}' here. Here's the corrected sentence: {cor_sent}.\".format(cor_tok=edit_item.c_str, orig_tok=edit_item.o_str, cor_sent=cor_sentence.text, orig_sent=orig_sentence.text)\n",
    "            llm_explanation = chain2.invoke({\"l1\": l1, \"level\": level, \"original\": orig_sentence, \"corrected\": cor_sentence})\n",
    "\n",
    "            line_1 = \"The verb tense you used in '{orig_sent}' isn't quite right. What would be a better tense of the verb '{lemma}' to use here?\".format(orig_sent=orig_sentence.text, lemma=helper_functions.get_lemma(edit_item))\n",
    "            response_1_correct = \"You got it! '{cor_tok}' makes more sense in this context. Remeber, make your verb tenses consistent.\".format(cor_tok=edit_item.c_str)\n",
    "            response_1_incorrect = \"You're still a little off. Remeber, you need to make your verb tenses consistent within and between sentences. Give it another try.\"\n",
    "            response_2_correct = \"Nice! That's exactly what you need. '{cor_tok}' is the right tense for this sentence.\".format(cor_tok=edit_item.c_str)\n",
    "            response_2_incorrect = \"Not quite. You should probably use '{cor_tok}' instead of '{orig_tok}' here. Here's the corrected sentence: {cor_sent}.\".format(cor_tok=edit_item.c_str, orig_tok=edit_item.o_str, cor_sent=cor_sentence.text)\n",
    "\n",
    "            out_dict['edit_' + str(error_count)] = {\"response_short\": response_short, \"llm_explanation\": llm_explanation, \"line_1\":line_1, \"response_1\":{'correct':response_1_correct, 'incorrect':response_1_incorrect}, 'response_2':{'correct':response_2_correct, 'incorrect':response_2_incorrect}}\n",
    "            error_count += 1\n",
    "\n",
    "        elif 'R:NOUN:NUM' in target:\n",
    "            #Don't know if I should include the full sentence\n",
    "            response_short = \"In '{orig_sent}' you used a {orig_number} noun where you should have used a {cor_number} noun. In this context, the noun {orig_tok} should be the {cor_number} noun {cor_tok}. Here's the corrected sentence: {cor_sent}.\".format(orig_sent=orig_sentence.text, orig_number=helper_functions.get_number(edit_item, 'orig'), cor_number=helper_functions.get_number(edit_item, 'cor'), cor_tok=edit_item.c_str, orig_tok=edit_item.o_str, cor_sent=cor_sentence.text)\n",
    "            llm_explanation = chain2.invoke({\"l1\": l1, \"level\": level, \"original\": orig_sentence, \"corrected\": cor_sentence})\n",
    "\n",
    "            line_1 = \"In '{orig_sent}' you used a {orig_number} noun where you should have used a {cor_number} noun. Can you spot the mistake? What would be the right noun form to use?\".format(orig_sent=orig_sentence.text, orig_number=helper_functions.get_number(edit_item, 'orig'), cor_number=helper_functions.get_number(edit_item, 'cor'))\n",
    "            response_1_correct = \"That's right! '{cor_tok}' should be plural in this context.\".format(cor_tok=edit_item.c_str)\n",
    "            response_1_incorrect = \"That's not the correction I was looking for. Remeber that when you're talking about things 'in general' (like movies or books) you often want to use a plural. Try one more time.\"\n",
    "            response_2_correct = \"Great! '{cor_tok}' should be plural in this context.\".format(cor_tok=edit_item.c_str)\n",
    "            response_2_incorrect = \"That's not the error I was thinking about. In this context, the noun {orig_tok} should be the {cor_number} noun {cor_tok}. Here's the corrected sentence: {cor_sent}.\".format(cor_tok=edit_item.c_str, cor_number=helper_functions.get_number(edit_item, 'cor'), orig_tok=edit_item.o_str, cor_sent=cor_sentence.text)\n",
    "\n",
    "            out_dict['edit_' + str(error_count)] = {\"response_short\": response_short, \"llm_explanation\": llm_explanation, \"line_1\":line_1, \"response_1\":{'correct':response_1_correct, 'incorrect':response_1_incorrect}, 'response_2':{'correct':response_2_correct, 'incorrect':response_2_incorrect}}\n",
    "            error_count += 1\n",
    "\n",
    "        elif 'R:VERB:FORM' in target:\n",
    "            #Don't know if I should include the full sentence\n",
    "            response_short = \"In '{orig_sent}' there's an issue with the form of the verb '{orig_tok}'. In this context, the verb '{orig_lemma}' should be the {cor_form} '{cor_tok}'. Here's the corrected sentence: {cor_sent}.\".format(orig_sent=orig_sentence.text, orig_tok=edit_item.o_str, cor_tok=edit_item.c_str, cor_form=helper_functions.get_verb_form(edit_item, 'cor'), orig_lemma=helper_functions.get_lemma(edit_item), cor_sent=cor_sentence.text)\n",
    "            llm_explanation = chain2.invoke({\"l1\": l1, \"level\": level, \"original\": orig_sentence, \"corrected\": cor_sentence})\n",
    "\n",
    "            line_1 = \"In '{orig_sent}' there's an issue with the form of the verb '{orig_tok}'. What would be a better form of this verb to use?\".format(orig_sent=orig_sentence.text, orig_tok=edit_item.o_str)\n",
    "            response_1_correct = \"Exactly! In this sentence you should have used the {cor_form} form '{cor_tok}' instead of the {orig_form} form '{orig_tok}'.\".format(cor_tok=edit_item.c_str, orig_tok=edit_item.o_str, cor_form=helper_functions.get_verb_form(edit_item, 'cor'), orig_form=helper_functions.get_verb_form(edit_item, 'orig'))\n",
    "            response_1_incorrect = \"Good try, but that's still a bit off. You should have used {cor_form} form of the verb '{orig_lemma}'. What would that form be?\".format(cor_form=helper_functions.get_verb_form(edit_item, 'cor'), orig_lemma=helper_functions.get_lemma(edit_item))\n",
    "            response_2_correct = \"Good job! That's the correct form I was looking for. Remember in English, we usually use participles after helping verbs like 'have' and 'is'.\"\n",
    "            response_2_incorrect = \"That's still not quite right. In this context, the verb '{orig_lemma}' should be the {cor_form} '{cor_tok}'. Here's the corrected sentence: {cor_sent}.\".format(cor_tok=edit_item.c_str, cor_form=helper_functions.get_verb_form(edit_item, 'cor'), orig_lemma=helper_functions.get_lemma(edit_item), cor_sent=cor_sentence.text)\n",
    "\n",
    "            out_dict['edit_' + str(error_count)] = {\"response_short\": response_short, \"llm_explanation\": llm_explanation, \"line_1\":line_1, \"response_1\":{'correct':response_1_correct, 'incorrect':response_1_incorrect}, 'response_2':{'correct':response_2_correct, 'incorrect':response_2_incorrect}}\n",
    "            error_count += 1\n",
    "\n",
    "        elif 'M:PREP' in target:\n",
    "            #Don't know if I should include the full sentence\n",
    "            response_short = \"You seem to be missing a preposition in the sentence '{orig_sent}.' You should probably add '{cor_tok}' to make the sentence sound more natural. Here's the corrected sentence: {cor_sent}\".format(orig_sent=orig_sentence.text, cor_tok=edit_item.c_str, cor_sent=cor_sentence.text)\n",
    "            llm_explanation = chain2.invoke({\"l1\": l1, \"level\": level, \"original\": orig_sentence, \"corrected\": cor_sentence})\n",
    "\n",
    "            line_1 = \"You seem to be missing a preposition in the sentence '{orig_sent}' How could you improve the sentence by adding a prepostion?\".format(orig_sent=orig_sentence.text)\n",
    "            response_1_correct = \"Yep, that's right. '{cor_tok}' is needed to make the sentence grammatical.\".format(cor_tok=edit_item.c_str)\n",
    "            response_1_incorrect = \"Not quite. Remember, a lot of fixed expressions require prepositions, like 'think about' or 'because of'. Try adding a preposition to the sentence one more time.\"\n",
    "            response_2_correct = \"Great! '{cor_tok}' is what the sentence is missing.\".format(cor_tok=edit_item.c_str)\n",
    "            response_2_incorrect = \"You're still missing something. You should probably add '{cor_tok}' to make the sentence sound more natural. Here's the corrected sentence: {cor_sent}\".format(cor_tok=edit_item.c_str, cor_sent=cor_sentence.text)\n",
    "\n",
    "            out_dict['edit_' + str(error_count)] = {\"response_short\": response_short, \"llm_explanation\": llm_explanation, \"line_1\":line_1, \"response_1\":{'correct':response_1_correct, 'incorrect':response_1_incorrect}, 'response_2':{'correct':response_2_correct, 'incorrect':response_2_incorrect}}\n",
    "            error_count += 1\n",
    "\n",
    "        elif 'U:PREP' in target:\n",
    "            #Don't know if I should include the full sentence\n",
    "            response_short = \"You seem to have included an unneeded preposition in the sentence '{orig_sent}'. In this context, you should drop the {orig_tok} before {next_tok}. Here's the corrected sentence: {cor_sent}\".format(orig_sent=orig_sentence.text, orig_tok=edit_item.o_str, next_tok=helper_functions.get_next_tok(edit_item, orig_sentence), cor_sent=cor_sentence.text)\n",
    "            llm_explanation = chain2.invoke({\"l1\": l1, \"level\": level, \"original\": orig_sentence, \"corrected\": cor_sentence})\n",
    "\n",
    "            line_1 = \"You seem to have included an unneeded preposition in the sentence '{orig_sent}'. How could you fix the sentence by removing a prepostion?\".format(orig_sent=orig_sentence.text)\n",
    "            response_1_correct = \"That's right. '{orig_tok}' isn't needed in this context, and it makes the sentence sound awkwark.\".format(orig_tok=edit_item.o_str)\n",
    "            response_1_incorrect = \"That's not what I was thinking of. Often, people add extra prepostions like to, of and by. Try rewording your sentence again.\"\n",
    "            response_2_correct = \"Excellent. Dropping the '{orig_tok}' definitely makes this sentence sound better.\".format(orig_tok=edit_item.o_str)\n",
    "            response_2_incorrect = \"That still sounds a little off. In this context, you should drop the {orig_tok} before {next_tok}. Here's the corrected sentence: {cor_sent}\".format(orig_tok=edit_item.o_str, next_tok=helper_functions.get_next_tok(edit_item, orig_sentence), cor_sent=cor_sentence.text)\n",
    "\n",
    "            out_dict['edit_' + str(error_count)] = {\"response_short\": response_short, \"llm_explanation\": llm_explanation, \"line_1\":line_1, \"response_1\":{'correct':response_1_correct, 'incorrect':response_1_incorrect}, 'response_2':{'correct':response_2_correct, 'incorrect':response_2_incorrect}}\n",
    "            error_count += 1\n",
    "\n",
    "\n",
    "    json_out = json.dumps(out_dict)\n",
    "\n",
    "    return json_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your course level:1\n",
      "Enter your native language (english|spanish|mandarin|other):english\n",
      "Write an essay in Spanish: Normalmente por vacaciones de Acción de Gracias, yo voy a la casa de mi tía en *STATE* para tres días. Mi papa tiene seis hermanos, así que yo tengo una familia grande. Yo vivo en *STATE* y mi familia vive lejos de me, entonces no los veo a menudo. El Día de Acción de Gracias es mi día de fiesta preferido porque yo puedo ver mi familia. Todo la familia va a la casa de mi tía. Este año no puedo ir a la casa de mi tía porque el coronavirus. Estuve muy triste, pero así que visito mi hermana menor en *CITY* con mi novio. Nosotros vimos tres películas, comimos mucho buena comida, y caminamos en la ciudad y el parque centro. Estuvimos muy feliz. ¡CITY is muy interesante! Tiene mucha gente differente. El Día de Acción de Gracias, quedamos en el apartamento pequeño de mi hermana. Solo teníamos una estufa y tres quemadores y tres personas, ¡pero hicimos un festín! Fue divertido y delicioso. Hicimos los panecillos, los boniatos horneados, el puré de papas, el pollo al horno, las tartes de champiñón, la salsa de arándano, y el relleno. ¡Que rico! Comimos la sobras para mucho días. También llamamos nos mama y papa y hablamos con ellos. Aunque ellos estan en *CITY*, sentí estamos cerca de nosotros. Aunque no puedo ir a la casa de mi tía con mi familia grande, este año estuve muy contento con mi familia pequeña. Todavía El Día de Acción de Gracias es mi día de fiesta preferido.\n",
      "['Normalmente por vacaciones de Acción de Gracias, voy a la casa de mi tía en *STATE * por tres días.']\n",
      "['Mi papá tiene seis hermanos, así que tengo una familia grande.']\n",
      "['Yo vivo en *STATE * y mi familia vive lejos de mí, entonces no los veo a menudo.']\n",
      "['El Día de Acción de Gracias es mi día de fiesta preferido porque puedo ver a mi familia.']\n",
      "['Todo la familia va a la casa de mi tía.']\n",
      "['Este año no puedo ir a la casa de mi tía porque el coronavirus.']\n",
      "['Estuve muy triste, pero así que visito a mi hermana menor en *CITY * con mi novio.']\n",
      "['Vimos tres películas, comimos mucha buena comida y caminamos en la ciudad y el parque centro.']\n",
      "['Estuvimos muy felices.']\n",
      "['¡CITY es muy interesante!']\n",
      "['Tiene mucha gente diferente.']\n",
      "['El Día de Acción de Gracias, quedamos en el apartamento pequeño de mi hermana.']\n",
      "['Solo teníamos una estufa y tres quemadores y tres personas, ¡pero hicimos un festín!']\n",
      "['Fue divertido y delicioso.']\n",
      "['Hicimos panecillos, los boniatos horneados, el puré de papas, el pollo al horno, las tartes de champiñón, la salsa de arándano y el relleno.']\n",
      "['¡Qué rico!']\n",
      "['Comimos la sobra para muchos días.']\n",
      "['También llamamos a nos mamá y papa y hablamos con ellos.']\n",
      "['Aunque ellos están en *CITY *, sentí que estamos cerca de nosotros.']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m edits, cor_lines \u001b[38;5;241m=\u001b[39m \u001b[43mgec\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_essay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m edits_to_present \u001b[38;5;241m=\u001b[39m rank_errors(edits)\n\u001b[1;32m     16\u001b[0m feedback \u001b[38;5;241m=\u001b[39m generate_feedback(edits_to_present, l1, level)\n",
      "Cell \u001b[0;32mIn[19], line 25\u001b[0m, in \u001b[0;36mgec\u001b[0;34m(input_essay)\u001b[0m\n\u001b[1;32m     22\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m orig_lines:\n\u001b[0;32m---> 25\u001b[0m     sent_results \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_grammar\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sent_results)\n\u001b[1;32m     27\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(sent_results)\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mcorrect_grammar\u001b[0;34m(input_text, num_return_sequences)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect_grammar\u001b[39m(input_text,num_return_sequences):\n\u001b[1;32m      2\u001b[0m     batch \u001b[38;5;241m=\u001b[39m tokenizer([input_text],truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch_device)\n\u001b[0;32m----> 3\u001b[0m     translated \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     tgt_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(translated, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tgt_text\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_serrant/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_serrant/lib/python3.9/site-packages/transformers/generation/utils.py:1834\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1826\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1827\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1828\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1829\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1830\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1831\u001b[0m     )\n\u001b[1;32m   1833\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1851\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1852\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1859\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_serrant/lib/python3.9/site-packages/transformers/generation/utils.py:3562\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3558\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m next_token_scores\u001b[38;5;241m.\u001b[39mview(batch_size, num_beams \u001b[38;5;241m*\u001b[39m vocab_size)\n\u001b[1;32m   3560\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 3562\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3563\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(next_token_scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, next_tokens)\n\u001b[1;32m   3565\u001b[0m next_token_scores, _indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(next_token_scores, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "level = input(\"Enter your course level:\")\n",
    "l1 = input(\"Enter your native language (english|spanish|mandarin|other):\")\n",
    "\n",
    "original = ''\n",
    "\n",
    "while True:\n",
    "    original_essay = input(\"Write an essay in Spanish: \")\n",
    "    \n",
    "    if original == 'exit':\n",
    "        break\n",
    "        \n",
    "    edits, cor_lines = gec(original_essay)\n",
    "    \n",
    "    edits_to_present = rank_errors(edits)\n",
    "    \n",
    "    feedback = generate_feedback(edits_to_present, l1, level)\n",
    "    \n",
    "    corrected_essay = ' '.join(cor_lines)\n",
    "    \n",
    "    print(f\"Original: {original_essay}\")\n",
    "    print(f\"Corrected: {corrected_essay}\")\n",
    "\n",
    "    print(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_serrant",
   "language": "python",
   "name": "torch_serrant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
